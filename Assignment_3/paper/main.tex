\documentclass{article}

\input{settings.tex}



\begin{document}
\selectlanguage{swedish}

\input{title.tex}



\section*{Problem 1}
%\newpage 
\subsection*{1.1a}
Neural network nr.1 can be trained using backpropagation. The input vector is propagated through the network until it reaches the output layer. After that it stops. The other networks don't move forward in each layer, either going backwards or to another neuron in the same layer.

\subsection*{1.1b}
In general an activation function in a neuron must be differentiable to use the backpropagation. It must be propagated through the network layer by layer until it reaches the output layer.

\subsection*{1.2}
The output has a linear activation function,

\begin{equation}
y = g(\bm{w}^T\bm{x}) = c \bm{w}^T \bm{x}.
\end{equation}

Combining two networks will then yield a new network of the same 
topology. 

\begin{align*}
	y & =g(\frac{1}{2} \bm{w}_1^T \bm{x}+\frac{1}{2} \bm{w}_2^{T} \bm{x}) = \text{As } g \text{ is linear} = \frac{1}{2}g(\bm{w}_1^T \bm{x})+\frac{1}{2}g(\bm{w}_2^T \bm{x}) = \frac{c}{2}\bm{w}_1^T\bm{x}+\frac{c}{2}\bm{w}_2^T\bm{x} \\ & = \frac{c}{2}(\bm{w}_1^T+\bm{w}_2^T)\bm{x} = c \bm{w}_3^T \bm{x} = g(\bm{w}_3^T\bm{x}).
\end{align*}

So combining these two networks will give us a new network with weights, $\bm{w}_3^T=\frac{1}{2}(\bm{w}_1^T+\bm{w}_2^T)$.


\subsection*{1.3}

Equation~\eqref{eq:step} shows how the weights are readjusted with the learning rate. 
\begin{equation}
\omega_i = \omega_{i-1} - \lambda\bigtriangledown E_n(\omega_i)
\label{eq:step}
\end{equation}
Knowing that the error is $E=\frac{1}{2}(t-y)^2$ the gradient of this error function is shown in equation~\eqref{eq:grad}
\begin{equation}
	\frac{\delta E_n}{\delta \omega_{ji}} = (t_{nj} - y_{nj})x_{ni} \rightarrow \bigtriangledown E_n(\textbf{w}) = (t - \textbf{w}^T\textbf{x})\textbf{x}
	\label{eq:grad}
\end{equation}
By combining equations~\eqref{eq:step} and~\eqref{eq:grad} we see that the term added to the readjusted weights from the algorithm are
\begin{equation*}
	-\lambda(t - \textbf{w}^T\textbf{x})\textbf{x}
	\label{eq:addterm}
\end{equation*}

\subsection*{1.4}

\begin{equation}
\frac{\partial E}{\partial z_k} = \frac{\partial E}{\partial y_k} \frac{\partial y_k}{\partial z_k} = \frac{\partial E}{\partial y_k} g'(z_k)
\end{equation}

\begin{equation}
\frac{\partial E}{\partial y_j} = \frac{\partial E}{\partial z_k} \frac{\partial z_k}{\partial y_j} =
\frac{\partial E}{\partial z_k} \frac{\partial}{\partial y_j} w_{jk} y_j =\frac{\partial E}{\partial z_k} w_{jk}
\end{equation}

\begin{equation}
\frac{\partial E}{\partial z_j} = \frac{\partial E}{\partial y_j} \frac{\partial y_j}{\partial z_j} = \frac{\partial E}{\partial y_k} g'(z_k) w_{jk} \frac{\partial y_j}{\partial z_j} = \frac{\partial E}{\partial y_k} g'(z_k) w_{jk} g'(z_j)
\end{equation}

\begin{equation}
\frac{\partial E}{\partial w_{jk}} = \frac{\partial E}{\partial z_k} \frac{\partial z_k}{\partial w_{jk}} = \frac{\partial E}{\partial y_k} g'(z_k) \frac{\partial}{\partial w_{jk}} w_{jk}y_{j} = \frac{\partial E}{\partial y_k} g'(z_k) \delta^j_k y_j,
\end{equation}

where $\delta^j_k$ is the delta function.

\begin{equation}
\frac{\partial E}{\partial w_{ij}} = \frac{\partial E}{\partial z_j} \frac{\partial z_j}{\partial w_{ij}} = \frac{\partial E}{\partial z_j} \frac{\partial}{\partial w_{ij}} w_{ij}y_i = \frac{\partial E}{\partial z_j} \delta^j_i y_i
\end{equation}


\section*{Problem 2}

For the calculations below there are implicit summations over indices that are found twice (simplified Einstein notation), e.g.

\begin{equation}
a_i b_i = \sum_{i=1}^{I} a_i b_i
\end{equation} 

\subsection*{Derivative cross-entropy error gradient with respoect to softmax $\sigma_C(\bm{z})$}

\begin{equation}
E = -t_i \log{y_i}
\end{equation}

\begin{equation}
\frac{\partial E}{\partial z_c} = -t_c \frac{\partial}{\partial z_c} \log {y_c} = -t_c\frac{\partial}{\partial z_c} \log{\sigma_c(\bm{z})} = -t_c \frac{\partial}{\partial z_c}\log{\frac{e^{z_c}}{\sum_{i}e^{z_i}}}
\end{equation}

\begin{equation}
\frac{\partial}{\partial z_c} \frac{e^{z_c}}{\sum_{i}e^{z_i}} = \frac{\partial}{\partial z_c} \left(z_c-\log{\sum_i e^{z_i}}\right) = 1-\frac{\partial}{\partial z_c} \log{\left[e^{z_c}+\sum_{i\neq c}e^{z_i}\right]} = 1-\frac{e^{z_c}}{\sum_{i}e^{z_i}} = 1-\sigma_c{\bm{z}}
\end{equation}

\begin{equation}
\frac{\partial E}{\partial z_c} = -t_c\left(1- \sigma_c(\bm{z}) \right) = \sigma_c(\bm{z})-t_c = y_c-t_c
\end{equation}

\subsection*{Derivative of Error function with respct to weight $w_{jk}$}

\begin{equation}
E = E^{Classification}+\alpha E^{weightdecay} = -\sum_{k}t_k\log{(\sigma_k{\bm{z}})} + \alpha \sum_j\sum_k ||w_{ij}||^2
\end{equation}

\begin{align*}
	\frac{\partial E}{\partial w_{jk}} &= \frac{\partial E^{Classification}}{\partial w_{jk}}+\alpha \frac{\partial E^{weightdecay}}{\partial w_{jk}} = \frac{\partial E}{\partial z_k}\frac{\partial z_k}{\partial w_{jk}} + \alpha \frac{\partial}{\partial w_{jk}} ||w_{jk}||^2\\
	&= \left(y_k-t_k\right)\delta^{k}_{j}y_j + \alpha 2w_{jk}
\end{align*}




\subsection*{Derivative of the logistic function $g(z)$}

\begin{equation}
g(z) = \frac{1}{1+e^{-z}}
\end{equation}

\begin{align}
	\frac{\partial g(z)}{\partial z} &= \frac{\partial}{\partial z} \frac{1}{1+e^{-z}} = \frac{e^{-z}}{(1+e^{-z})^2} = \frac{1+e^{-z}}{(1+e^{-z})^2}-\frac{1}{(1+e^{-z})^2}= \frac{1}{1+e^{-z}}\left(1-\frac{1}{1+e^{-z}}\right)\\ &=g(z)(1-g(z))
\end{align}

\subsection*{Derivative of Error function with respect to weight $w_{ij}$}

\begin{equation}
\frac{\partial E}{\partial w{ij}} = \frac{\partial E^{Classification}}{\partial w_{ij}} + \alpha \frac{\partial E^{weightdecay}}{\partial w_{ij}}
\end{equation}

\begin{align*}
	\frac{\partial E^{Classification}}{\partial w_{ij}} &= \frac{\partial E}{\partial z_k}\frac{\partial z_k}{\partial y_j}\frac{\partial y_j}{\partial z_j}\frac{\partial z_j}{\partial w_{ij}} = (y_k-t_k)w_{jk} g'(w_{ij}x_i) x_i \\&= (y_k-t_k)w_{jk}g(w_{ij}x_i)(1-g(w_{ij}x_i))x_i
\end{align*}

\begin{align*}
	\frac{\partial E^{weightdecay}}{\partial w_{ij}} &= \frac{\partial}{\partial w_{ij}} ||w_{ij}||^2 \\&= 2\alpha w_{ij}
\end{align*}

\begin{equation}
\frac{\partial E}{\partial w_{ij}} = (y_k-t_k)w_{jk} g'(w_{ij}x_i) x_i = (y_k-t_k)w_{jk}g(w_{ij}x_i)(1-g(w_{ij}x_i))x_i+2\alpha w_{ij}
\end{equation}


\end{document}
